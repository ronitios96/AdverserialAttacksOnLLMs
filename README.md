# Universal and Transferable Adversarial Attacks on Aligned Language Models

This repository accompanies the final ML project submission by Ronit Tushir and Ambica Bhatia (NYU Tandon, Spring 2025). It explores and replicates the adversarial attack framework from the paper:

> **Universal and Transferable Adversarial Attacks on Aligned Language Models**  
> Zou et al., 2023 â€” [arXiv:2307.15043](https://arxiv.org/abs/2307.15043)

---

## ğŸ”¬ Project Objective

We reproduce and critically evaluate the **Greedy Coordinate Gradient (GCG)** attack for aligned LLMs (e.g., ChatGPT, Claude) and explore a high-efficiency extension known as **nanoGCG**. The core hypothesis tested is:

> *A single, universal adversarial suffixâ€”optimized on a base LLMâ€”can transfer to and jailbreak aligned instruction-following models.*

---

## ğŸ“ Repository Structure

/notebooks/
â”œâ”€â”€ Main_Notebook.ipynb # Full theory walkthrough + GCG Demo + nanoGCG Demo + OpenAI Client
â”œâ”€â”€ GCG_Demo.ipynb # Focused implementation of GCG based on official llm-attacks repo
â”œâ”€â”€ nanoGCG_Demo.ipynb # AdvBench-based evaluation using Gray Swan's nanoGCG
â”œâ”€â”€ OpenAI_Client_Prompter.ipynb # Minimal prompt tester for OpenAI's GPT-3.5/GPT-4



---

## ğŸ“Š Notable Results

- GCG achieves ~65â€“75% success on fixed harmful prompts (e.g., death threats) within 20 minutes.
- nanoGCG matches performance on randomized AdvBench prompts in under 15 minutes â€” enabling efficient, scalable adversarial testing.
- Transferability to GPT-3.5 observed; GPT-4 and Claude show near-zero success due to stronger alignment.

---

## ğŸ§  Highlights

- Mathematical breakdown and loss function formulation for GCG
- Attack and optimization loop explained with code
- nanoGCG benchmarking on randomized harmful prompts
- Comparative analysis of GCG vs nanoGCG vs AutoPrompt vs AutoDAN
- Full test infrastructure for reproducibility and extension

---

## âš ï¸ Disclaimer

All experiments were conducted in an academic context for educational and research purposes. No adversarial outputs were used or shared outside the testing environment. The results are intended to analyze the safety and alignment robustness of modern LLMs, not to exploit or disseminate harmful behavior.

---

## ğŸ“„ License

This repository is open for academic use. If using any portion of this project, please cite the original paper and reference this repository.

