{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA1NHx3_u_Ee"
      },
      "outputs": [],
      "source": [
        "#Install Dependencies\n",
        "!git clone https://github.com/GraySwanAI/nanoGCG.git\n",
        "%cd nanoGCG\n",
        "!pip install nanogcg\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Log on to HF\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Replace YOUR_HF_TOKEN with your actual Hugging Face token\n",
        "login(\"\")\n"
      ],
      "metadata": {
        "id": "iZT4TSJ4vAvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load worst prompts from AdvBench Data Set, and then pick a random prompt\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the full AdvBench dataset\n",
        "dataset = load_dataset(\"walledai/AdvBench\", split=\"train\")\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import nanogcg\n",
        "from nanogcg import GCGConfig\n",
        "\n",
        "# Load one random sample from AdvBench\n",
        "dataset = load_dataset(\"walledai/AdvBench\", split=\"train\")\n",
        "keywords = [\"kill\", \"attack\", \"bomb\", \"hack\", \"steal\", \"murder\", \"rob\"]\n",
        "def keyword_score(text):\n",
        "    return sum(kw in text.lower() for kw in keywords)\n",
        "\n",
        "worst_10 = sorted(dataset, key=lambda x: keyword_score(x[\"target\"]), reverse=True)[:10]\n",
        "\n",
        "example = random.choice(worst_10)\n",
        "prompt = example[\"prompt\"]\n",
        "target = example[\"target\"]\n",
        "\n",
        "print(\"ðŸ§¨ Prompt:\", prompt)\n",
        "print(\"ðŸŽ¯ Target:\", target)"
      ],
      "metadata": {
        "id": "JXSdFtKqvCiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the actual model + tokenizer\n",
        "print(\"ðŸ§¨ Prompt:\", prompt)\n",
        "print(\"ðŸŽ¯ Target:\", target)\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "vkSFEjFgvEHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the prompt - Should be denied till here (Some Adv Prompts work because model doesn't block them, shuffle the adv prompt from the above code to find one that doesn't work and attack using nanoGCG)"
      ],
      "metadata": {
        "id": "c319T-fBvJMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adv_prompt = prompt\n",
        "inputs = tokenizer(adv_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(**inputs, max_new_tokens=200)\n",
        "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nðŸ“¤ Final Model Output:\\n\", decoded)"
      ],
      "metadata": {
        "id": "5r-tJjdjvFR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running nanoGCG with only 150 steps\n",
        "config = GCGConfig(\n",
        "    num_steps=150,\n",
        "    search_width=512,\n",
        "    topk=256,\n",
        "    n_replace=2,\n",
        "    buffer_size=10,\n",
        "    use_mellowmax=True,\n",
        "    mellowmax_alpha=1.0,\n",
        "    early_stop=True,\n",
        "    seed=42,\n",
        "    filter_ids=False  # disable filtering for max flexibility\n",
        ")\n",
        "\n",
        "\n",
        "result = nanogcg.run(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    target,\n",
        "    config\n",
        ")"
      ],
      "metadata": {
        "id": "lqTbQIShvG8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompting the surrogate model with Adv Suffix (should work here now)"
      ],
      "metadata": {
        "id": "d-pfOV_avPew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nâœ… Optimized Suffix:\\n\", result.best_string)\n",
        "\n",
        "adv_prompt = prompt + \" \" + result.best_string\n",
        "inputs = tokenizer(adv_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(**inputs, max_new_tokens=200)\n",
        "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nðŸ“¤ Final Model Output:\\n\", decoded)"
      ],
      "metadata": {
        "id": "mcKT3MOlvR8c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}